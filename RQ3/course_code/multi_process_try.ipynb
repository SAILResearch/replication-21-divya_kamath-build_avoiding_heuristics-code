{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ce4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from numpy import sqrt\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from matplotlib import pyplot\n",
    "from statistics import median\n",
    "import pickle\n",
    "import csv\n",
    "import warnings\n",
    "import datetime\n",
    "import multiprocessing\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2180e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_list = ['geoserver', 'gradle', 'cloud_controller_ng', 'opal', 'jruby', 'cloudify', 'chef', 'orbeon-forms', 'vagrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfddd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_values(Y_data):\n",
    "    Y_t = []\n",
    "    for e in Y_data:\n",
    "        if e == 'passed':\n",
    "            Y_t.append(1)\n",
    "        else:\n",
    "            Y_t.append(0) \n",
    "    return Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0935f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pass_streak(y_project):\n",
    "    p = y_project[0]\n",
    "    pass_streak = [y_project[0]]\n",
    "    for i in range(1, len(y_project)):\n",
    "        pass_streak.append(p)\n",
    "        if y_project[i] == 1:\n",
    "            p += 1\n",
    "        else:\n",
    "            p = 0\n",
    "    return pass_streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d9aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_failures(df):\n",
    "    \n",
    "    results = df['tr_status'].tolist()\n",
    "    length = len(results)\n",
    "    verdict = ['keep']\n",
    "    prev = results[0]\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        if results[i] == 0:\n",
    "            if prev == 0:\n",
    "                verdict.append('discard')\n",
    "                #print(i+1)\n",
    "            else:\n",
    "                verdict.append('keep')\n",
    "        else:\n",
    "            verdict.append('keep')\n",
    "        prev = results[i]\n",
    "    \n",
    "    df['verdict'] = verdict\n",
    "    df = df[ df['verdict'] == 'keep' ]\n",
    "    df.drop('verdict', inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e600e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_data(p_name):\n",
    "    \n",
    "    #open the metrics file\n",
    "    filename = 'metrics_data/' + p_name + '_metrics.csv'\n",
    "    project = pd.read_csv(filename)\n",
    "    \n",
    "    #clean the data & remove correlated columns\n",
    "    project = project [ project['developer_experience'] >= 0]\n",
    "    project.drop('num_commits', inplace=True, axis=1)\n",
    "    project.drop('reviewer_experience', inplace=True, axis=1)\n",
    "    project.drop('num_of_reviewers', inplace=True, axis=1)\n",
    "    \n",
    "    build_ids = project['tr_build_id'].tolist()\n",
    "    #get results data\n",
    "    res_file = '../data/' + p_name + '.csv'\n",
    "    res_project = pd.read_csv(res_file, usecols = ['tr_build_id', 'gh_build_started_at', 'tr_status'])\n",
    "    res_project['gh_build_started_at'] =  pd.to_datetime(res_project['gh_build_started_at'], format='%Y-%m-%d %H:%M:%S')\n",
    "    y_project = res_project[res_project['tr_build_id'].isin(build_ids)]['tr_status'].tolist()\n",
    "    y_project = output_values(y_project)\n",
    "    \n",
    "    #append date of build\n",
    "    project_dates = res_project[res_project['tr_build_id'].isin(build_ids)]['gh_build_started_at'].tolist()\n",
    "    project['gh_build_started_at'] = project_dates\n",
    "    \n",
    "    #add results column to the dataframe\n",
    "    project['tr_status'] = y_project\n",
    "    \n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7255c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_date(project):\n",
    "    dates = project['gh_build_started_at'].tolist()\n",
    "    \n",
    "    start_date = dates[0] - datetime.timedelta(days = 1)\n",
    "    end_date = dates[-1] - datetime.timedelta(days = 1)\n",
    "    \n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae6a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_data(p_name, build_ids):\n",
    "    \n",
    "    res_file = '../data/' + p_name + '.csv'\n",
    "    res_project = pd.read_csv(res_file, usecols = ['tr_build_id', 'tr_duration'])\n",
    "    durations = res_project[res_project['tr_build_id'].isin(build_ids)]['tr_duration'].tolist()\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edff3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(p_name, test_builds, test_result, pred_result):\n",
    "    \n",
    "    \n",
    "    \n",
    "    durations = get_required_data(p_name, test_builds)\n",
    "    actual_duration = sum(durations)\n",
    "    actual_failures = test_result.count(0)\n",
    "    \n",
    "    total_builds = len(test_builds)\n",
    "    num_of_builds = 0\n",
    "    total_duration = 0\n",
    "    cbf = 0\n",
    "    saved_builds = 0\n",
    "    \n",
    "    batch = []\n",
    "    batch_duration = []\n",
    "    actual_results = []\n",
    "    max_batch_size = 4\n",
    "    \n",
    "    for i in range(len(pred_result)):\n",
    "        if pred_result[i] == 0:\n",
    "            \n",
    "            if test_result[i] == 0:\n",
    "                cbf += 1\n",
    "                \n",
    "            if len(batch) < max_batch_size:\n",
    "                batch.append(pred_result[i])\n",
    "                batch_duration.append(durations[i])\n",
    "                actual_results.append(test_result[i])\n",
    "            \n",
    "            if len(batch) == max_batch_size:\n",
    "                num_of_builds += 1\n",
    "                total_duration += max(batch_duration)\n",
    "                \n",
    "                if 0 in actual_results:\n",
    "                    num_of_builds += 4\n",
    "                    total_duration += sum(batch_duration)\n",
    "        else:\n",
    "            saved_builds += 1\n",
    "            \n",
    "    if len(batch) > 0:\n",
    "        num_of_builds += 1\n",
    "        total_duration += max(batch_duration)\n",
    "        \n",
    "        if 0 in actual_results:\n",
    "            num_of_builds += len(batch)\n",
    "            total_duration += sum(batch_duration)\n",
    "                    \n",
    "    #Delay computation\n",
    "    flag = 0\n",
    "    count = 0\n",
    "    delay = []\n",
    "    for i in range(len(pred_result)):\n",
    "        if flag == 1:\n",
    "            if pred_result[i] == 1:\n",
    "                count += 1\n",
    "            \n",
    "            if pred_result[i] == 0:\n",
    "                delay.append(count)\n",
    "                count = 0\n",
    "                flag = 0\n",
    "                \n",
    "        if test_result[i] != 1:\n",
    "            if pred_result[i] == 1:\n",
    "                flag = 1\n",
    "    delay.append(count)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        time_saved = 100*total_duration/actual_duration\n",
    "        builds_saved = 100*saved_builds/total_builds\n",
    "        reqd_builds = 100*num_of_builds/total_builds\n",
    "        failed = 100*cbf/actual_failures\n",
    "        median_delays = median(delay)\n",
    "        total_delays = sum(delay)\n",
    "    \n",
    "#         print(\"===========================================\")\n",
    "#         print('The performance of the model is as follows:')\n",
    "#         print('\\t Time Reqd : {}'.format(total_duration))\n",
    "#         print('\\t % Time Reqd : {}%'.format(time_saved))\n",
    "#         print('\\t Num. Builds saved : {}%'.format(saved_builds))\n",
    "#         print('\\t % Builds saved : {}%'.format(builds_saved))\n",
    "#         print('\\t Num. Builds required : {}'.format(num_of_builds))\n",
    "#         print('\\t % Builds required : {}%'.format(reqd_builds))\n",
    "#         print('\\t Num. Failed Builds Identified : {}'.format(cbf))\n",
    "#         print('\\t % Failed Builds Identified : {}%'.format(failed))\n",
    "#         print('\\t Median Delay Induced : {} builds'.format(median_delays))\n",
    "#         print('\\t Total Delay Induced: {} builds'.format(total_delays))\n",
    "#         print('\\t Total number of builds: {}'.format(total_builds))\n",
    "#         print('\\t Total number of failed builds: {}'.format(actual_failures))\n",
    "#         print('\\t Total Duration: {}'.format(actual_duration))\n",
    "#         print(\"===========================================\")\n",
    "        \n",
    "    except:\n",
    "        \n",
    "#         print('exception')\n",
    "        return (0, 0, 0, 0, 0, 0)\n",
    "    \n",
    "    return (time_saved, builds_saved, reqd_builds, failed, median_delays, total_delays)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769fafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(p_name):\n",
    "    \n",
    "    performances = {'time_saved':[], 'builds_saved':[], 'builds_reqd':[], 'failed_builds':[], 'total_delay':[], 'median_delay':[]}\n",
    "#     print('Processing {}'.format(p_name))\n",
    "    \n",
    "    project = get_complete_data(p_name)\n",
    "    start_date, end_date = get_start_end_date(project)\n",
    "    \n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "    \n",
    "    param_grid = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "    forest = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = forest, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 0)\n",
    "    \n",
    "    phase = 1\n",
    "\n",
    "    while start_date < end_date:\n",
    "        \n",
    "        train_period = 100\n",
    "        test_period = 10\n",
    "        \n",
    "        while True:\n",
    "            train_end = start_date + datetime.timedelta(days = train_period + 1)\n",
    "            test_start = start_date + datetime.timedelta(days = train_period)\n",
    "            test_end = test_start + datetime.timedelta(days = test_period)\n",
    "\n",
    "            #getting data of train & test phase wise\n",
    "            train_data = project[ (project['gh_build_started_at'] > start_date) & (project['gh_build_started_at'] < train_end)]\n",
    "            test_data = project[ (project['gh_build_started_at'] > test_start) & (project['gh_build_started_at'] < test_end)]\n",
    "\n",
    "            #getting 'y' data\n",
    "            train_result = train_data['tr_status'].tolist()\n",
    "            test_result = test_data['tr_status'].tolist()\n",
    "            \n",
    "            if len(train_result) > 100 and len(test_result) > 10 :\n",
    "                break\n",
    "            \n",
    "            if len(train_result) <= 100:\n",
    "                train_period += 20\n",
    "            \n",
    "            if len(test_result) <= 10:\n",
    "                test_period += 20\n",
    "                \n",
    "            \n",
    "        \n",
    "        #dropping build start time column\n",
    "        train_data.drop('gh_build_started_at', inplace=True, axis=1)\n",
    "        test_data.drop('gh_build_started_at', inplace=True, axis=1)\n",
    "        \n",
    "        #add pass_streak to training data:\n",
    "        train_data['num_of_passes'] = get_pass_streak(train_result)\n",
    "        \n",
    "        best_n_estimators = []\n",
    "        best_max_depth = []\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_f1_sample = 0\n",
    "        best_f1_sample_result = 0\n",
    "        best_f1_estimator = 0\n",
    "        best_thresholds = []\n",
    "        \n",
    "#         train_result = train_data['tr_status']\n",
    "        \n",
    "#         train_data.drop('tr_status', inplace=True, axis=1)\n",
    "#         train_data.drop('tr_status', inplace=True, axis=1)\n",
    "        \n",
    "#         test_data.drop('tr_build_id', inplace=True, axis=1)\n",
    "#         test_data.drop('tr_build_id', inplace=True, axis=1)\n",
    "        \n",
    "#         grid_search.fit(train_data, train_result)\n",
    "        \n",
    "        #bootstrap 10 times\n",
    "        for i in range(2):\n",
    "            \n",
    "            file_name = 'rq2_' + p_name + '_' + str(phase) + '_model_' + str(i+1) + '_model.pkl'\n",
    "            sample_train = resample(train_data, replace=True, n_samples=len(train_data))\n",
    "            sample_train_result = sample_train['tr_status']\n",
    "            \n",
    "            build_ids = sample_train['tr_build_id'].tolist()\n",
    "            sample_test = train_data [~train_data['tr_build_id'].isin(build_ids)] \n",
    "            sample_test_result = sample_test['tr_status']\n",
    "            \n",
    "            #dropping result column and build ids column\n",
    "            sample_train.drop('tr_status', inplace=True, axis=1)\n",
    "            sample_train.drop('tr_build_id', inplace=True, axis=1)\n",
    "            sample_test.drop('tr_status', inplace=True, axis=1)\n",
    "            sample_test.drop('tr_build_id', inplace=True, axis=1)\n",
    "            \n",
    "            #training\n",
    "            grid_search.fit(sample_train, sample_train_result)\n",
    "            sample_pred_vals = grid_search.predict_proba(sample_test)\n",
    "            \n",
    "            pred_vals = sample_pred_vals[:, 1]\n",
    "            fpr, tpr, t = roc_curve(sample_test_result, pred_vals)\n",
    "            gmeans = sqrt(tpr * (1-fpr))\n",
    "            ix = argmax(gmeans)\n",
    "            bt = t[ix]\n",
    "            best_thresholds.append(bt)\n",
    "            \n",
    "            final_pred_result = []\n",
    "            #threshold setting\n",
    "            for j in range(len(pred_vals)):\n",
    "                if pred_vals[j] > bt:\n",
    "                    final_pred_result.append(1)\n",
    "                else:\n",
    "                    final_pred_result.append(0)\n",
    "            \n",
    "            try:\n",
    "                accuracy = accuracy_score(sample_test_result, final_pred_result)\n",
    "                precision = precision_score(sample_test_result, final_pred_result)\n",
    "                recall = recall_score(sample_test_result, final_pred_result)\n",
    "                confusion = confusion_matrix(sample_test_result, final_pred_result)\n",
    "                auc_score = roc_auc_score(sample_test_result, final_pred_result)\n",
    "                f1 = f1_score(sample_test_result, final_pred_result)\n",
    "            except:\n",
    "                print('')\n",
    "    \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_f1_sample = sample_train\n",
    "                best_f1_sample_result = sample_train_result\n",
    "                best_f1_estimator = grid_search.best_estimator_\n",
    "\n",
    "#             print(precision, recall, accuracy, f1, auc_score)\n",
    "            best_n_estimators.append(grid_search.best_params_['n_estimators'])\n",
    "            best_max_depth.append(grid_search.best_params_['max_depth'])\n",
    "        \n",
    "        #completed with bootstrapping \n",
    "        threshold = median(best_thresholds)\n",
    "        n_estimator = median(best_n_estimators)\n",
    "        max_depth = median(best_max_depth)\n",
    "        #retrain on the best \n",
    "        forest = RandomForestClassifier(n_estimators=int(n_estimator), max_depth=int(max_depth))\n",
    "        forest.fit(best_f1_sample, best_f1_sample_result)\n",
    "        \n",
    "        test_builds = test_data['tr_build_id'].tolist()\n",
    "        test_data.drop('tr_build_id', inplace=True, axis=1)\n",
    "        test_data.drop('tr_status', inplace=True, axis=1)\n",
    "        \n",
    "        final_pred_result = []\n",
    "        queue = 0\n",
    "        i = 0\n",
    "        total = len(test_data)\n",
    "        while i < total :\n",
    "            data = test_data.iloc[i]\n",
    "            data['num_of_passes'] = queue\n",
    "            predict = forest.predict_proba([data])\n",
    "            if predict[0][1] > threshold:\n",
    "                final_pred_result.append(1)\n",
    "                queue += 1\n",
    "                i+=1\n",
    "            else:\n",
    "                final_pred_result.append(0)\n",
    "                queue = 0\n",
    "                i += 1\n",
    "                \n",
    "                \n",
    "#         print('Individual testing for {}....'.format(p_name))\n",
    "        \n",
    "        try:\n",
    "            accuracy = accuracy_score(test_result, final_pred_result)\n",
    "            precision = precision_score(test_result, final_pred_result)\n",
    "            recall = recall_score(test_result, final_pred_result)\n",
    "            confusion = confusion_matrix(test_result, final_pred_result)\n",
    "            auc_score = roc_auc_score(test_result, final_pred_result)\n",
    "            f1 = f1_score(test_result, final_pred_result)\n",
    "\n",
    "#             print(precision, recall, accuracy, f1, auc_score)\n",
    "#             print(confusion)\n",
    "            \n",
    "        except:\n",
    "            print('')\n",
    "        \n",
    "        batch_performance = compute_performance(p_name, test_builds, test_result, final_pred_result)\n",
    "        \n",
    "        performances['time_saved'].append(batch_performance[0])\n",
    "        performances['builds_saved'].append(batch_performance[1])\n",
    "        performances['builds_reqd'].append(batch_performance[2])\n",
    "        performances['failed_builds'].append(batch_performance[3])\n",
    "        performances['median_delay'].append(batch_performance[4])\n",
    "        performances['total_delay'].append(batch_performance[5])\n",
    "        \n",
    "        start_date = test_end\n",
    "        phase += 1\n",
    "    \n",
    "    \n",
    "    #Project Performance:\n",
    "    \n",
    "    print(\"Average Time Saved in {} = {}\".format(p_name, sum(performances['time_saved'])/len(performances['time_saved'])))\n",
    "    print(\"Average Builds Saved in {} = {}\".format(p_name, sum(performances['builds_saved'])/len(performances['builds_saved'])))\n",
    "    print(\"Average Builds Reqd in {} = {}\".format(p_name, sum(performances['builds_reqd'])/len(performances['builds_reqd'])))\n",
    "    print(\"Average Failed Identified in {} = {}\".format(p_name, sum(performances['failed_builds'])/len(performances['failed_builds'])))\n",
    "    print(\"Average Median Delay in {} = {}\".format(p_name, sum(performances['median_delay'])/len(performances['median_delay'])))\n",
    "    print(\"Average Total Delay in {} = {}\".format(p_name, sum(performances['total_delay'])/len(performances['total_delay'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5976a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing geoserver\n",
      "<class 'numpy.ndarray'>\n",
      "[0.04947222 0.11111111 0.15388889 0.42766402 0.78722222 0.54888889\n",
      " 0.37444444 0.67222222 0.26671296 0.04947222 0.01350926 0.31482407\n",
      " 0.06884259 0.42273148 0.34555556 0.89713492 0.01350926 0.27518519\n",
      " 0.86244444 0.97496429 0.78722222 0.165      0.83611111 0.81381481\n",
      " 0.91093519 0.94380556 0.3706541  0.37131504 0.73888889 0.47293254\n",
      " 0.38368056 0.36359259 0.07277778 0.93538889 0.99362169 0.04333333\n",
      " 0.57666667 0.93277778 0.98010317 0.98690873 0.99       0.98444444\n",
      " 0.95777778 0.99444444 0.56111111 0.49388889 0.91166667 0.57055556\n",
      " 0.90833333 0.88388889 0.53833333 0.355      0.3751034  0.60833333\n",
      " 0.67333333 0.60643519 0.36605093 0.29222222 0.31555556 0.555\n",
      " 0.365      0.21777778 0.71333333 0.45962963 0.26666667 0.41\n",
      " 0.9110754  0.42       0.24388889 0.58833333 0.34833333 0.13222222\n",
      " 0.12055556 0.36106481 0.59722222 0.36944444 0.68611111]\n",
      "0.5294117647058824 0.4864864864864865 0.5454545454545454 0.5070422535211269 0.5432432432432432\n",
      "<class 'numpy.ndarray'>\n",
      "[0.18666667 0.675      0.095      0.675      0.76166667 0.37833333\n",
      " 0.40833333 0.02833333 0.06666667 0.07166667 0.325      0.225\n",
      " 0.43166667 0.51166667 0.415      0.24333333 0.74666667 0.02833333\n",
      " 0.76666667 0.765      0.2        0.565      0.39666667 0.385\n",
      " 0.17       0.80666667 0.29333333 0.84666667 0.575      0.38666667\n",
      " 0.67166667 0.98166667 0.56166667 0.17166667 0.685      0.32833333\n",
      " 0.34666667 0.99       0.98833333 0.92       0.40166667 0.37333333\n",
      " 0.98       0.82666667 0.44       0.30166667 0.99       0.73333333\n",
      " 0.39       0.81666667 0.42333333 0.99       0.505      0.22\n",
      " 0.22666667 0.74666667 0.23666667 0.32833333 0.54166667 0.145\n",
      " 0.63       0.47833333 0.325      0.16833333 0.69166667 0.52833333\n",
      " 0.73833333 0.07166667 0.54333333 0.865      0.25833333]\n",
      "0.6 0.32432432432432434 0.5352112676056338 0.4210526315789474 0.5445151033386327\n",
      "Individual testing for geoserver....\n",
      "0.0 0.0 0.8 0.0 0.4444444444444444\n",
      "[[24  3]\n",
      " [ 3  0]]\n",
      "===========================================\n",
      "The performance of the model is as follows:\n",
      "\t Time Reqd : 353300\n",
      "\t % Time Reqd : 561.1142875293818%\n",
      "\t Num. Builds saved : 3%\n",
      "\t % Builds saved : 10.0%\n",
      "\t Num. Builds required : 125\n",
      "\t % Builds required : 416.6666666666667%\n",
      "\t Num. Failed Builds Identified : 24\n",
      "\t % Failed Builds Identified : 88.88888888888889%\n",
      "\t Median Delay Induced : 0 builds\n",
      "\t Total Delay Induced: 1 builds\n",
      "\t Total number of builds: 30\n",
      "\t Total number of failed builds: 27\n",
      "\t Total Duration: 62964\n",
      "===========================================\n",
      "<class 'numpy.ndarray'>\n",
      "[2.90000000e-02 0.00000000e+00 9.27500000e-02 0.00000000e+00\n",
      " 0.00000000e+00 1.75000000e-02 1.55000000e-02 0.00000000e+00\n",
      " 2.04000000e-01 8.00000000e-03 6.80000000e-02 0.00000000e+00\n",
      " 0.00000000e+00 5.00000000e-04 2.00000000e-02 6.78571429e-03\n",
      " 2.00000000e-02 7.96666667e-02 0.00000000e+00 0.00000000e+00\n",
      " 1.75000000e-02 2.75000000e-02 0.00000000e+00 0.00000000e+00\n",
      " 8.50000000e-02 5.75000000e-02 0.00000000e+00 2.40000000e-02\n",
      " 0.00000000e+00 5.12500000e-02 3.75000000e-03 7.12500000e-02\n",
      " 0.00000000e+00 6.81666667e-02 0.00000000e+00 3.02500000e-01\n",
      " 2.72500000e-01 1.25000000e-03 2.65000000e-01 2.50000000e-02\n",
      " 0.00000000e+00 7.25000000e-02 1.50000000e-02 0.00000000e+00\n",
      " 0.00000000e+00 2.00000000e-03 1.66666667e-02 2.80000000e-01\n",
      " 3.75000000e-03 6.75000000e-02 1.79166667e-02 1.30000000e-01\n",
      " 0.00000000e+00 0.00000000e+00 1.28833333e-01 3.27500000e-02\n",
      " 1.50000000e-02 5.00000000e-03 2.32500000e-01 0.00000000e+00\n",
      " 2.50000000e-03 5.75000000e-02 3.13833333e-01 2.32500000e-01\n",
      " 2.25000000e-02 1.17500000e-01 1.00000000e-01 7.32500000e-01\n",
      " 1.17500000e-01 2.07500000e-01]\n",
      "0.06451612903225806 0.6666666666666666 0.5714285714285714 0.1176470588235294 0.6169154228855721\n"
     ]
    }
   ],
   "source": [
    "for p_name in project_list:\n",
    "    \n",
    "\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
