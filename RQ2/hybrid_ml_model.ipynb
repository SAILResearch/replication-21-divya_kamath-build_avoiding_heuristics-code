{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ce4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from numpy import sqrt\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from matplotlib import pyplot\n",
    "from statistics import median\n",
    "import pickle\n",
    "import csv\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2180e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_list = ['geoserver', 'gradle', 'cloud_controller_ng', 'opal', 'jruby', 'cloudify', 'chef', 'orbeon-forms', 'vagrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfddd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_values(Y_data):\n",
    "    Y_t = []\n",
    "    for e in Y_data:\n",
    "        if e == 'passed':\n",
    "            Y_t.append(1)\n",
    "        else:\n",
    "            Y_t.append(0) \n",
    "    return Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0935f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pass_streak(y_project):\n",
    "    p = y_project[0]\n",
    "    pass_streak = [y_project[0]]\n",
    "    for i in range(1, len(y_project)):\n",
    "        pass_streak.append(p)\n",
    "        if y_project[i] == 1:\n",
    "            p += 1\n",
    "        else:\n",
    "            p = 0\n",
    "    return pass_streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d9aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_failures(df):\n",
    "    \n",
    "    results = df['tr_status'].tolist()\n",
    "    length = len(results)\n",
    "    verdict = ['keep']\n",
    "    prev = results[0]\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        if results[i] == 0:\n",
    "            if prev == 0:\n",
    "                verdict.append('discard')\n",
    "                #print(i+1)\n",
    "            else:\n",
    "                verdict.append('keep')\n",
    "        else:\n",
    "            verdict.append('keep')\n",
    "        prev = results[i]\n",
    "    \n",
    "    df['verdict'] = verdict\n",
    "    df = df[ df['verdict'] == 'keep' ]\n",
    "    df.drop('verdict', inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e600e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_data(p_name):\n",
    "    \n",
    "    #open the metrics file\n",
    "    filename = 'metrics_data/' + p_name + '_metrics.csv'\n",
    "    project = pd.read_csv(filename)\n",
    "    \n",
    "    #clean the data & remove correlated columns\n",
    "    project = project [ project['developer_experience'] >= 0]\n",
    "    project.drop('num_commits', inplace=True, axis=1)\n",
    "    project.drop('reviewer_experience', inplace=True, axis=1)\n",
    "    project.drop('num_of_reviewers', inplace=True, axis=1)\n",
    "    \n",
    "    build_ids = project['tr_build_id'].tolist()\n",
    "    #get results data\n",
    "    res_file = '../data/' + p_name + '.csv'\n",
    "    res_project = pd.read_csv(res_file, usecols = ['tr_build_id', 'gh_build_started_at', 'tr_status'])\n",
    "    res_project['gh_build_started_at'] =  pd.to_datetime(res_project['gh_build_started_at'], format='%Y-%m-%d %H:%M:%S')\n",
    "    y_project = res_project[res_project['tr_build_id'].isin(build_ids)]['tr_status'].tolist()\n",
    "    y_project = output_values(y_project)\n",
    "    \n",
    "    #append date of build\n",
    "    project_dates = res_project[res_project['tr_build_id'].isin(build_ids)]['gh_build_started_at'].tolist()\n",
    "    project['gh_build_started_at'] = project_dates\n",
    "    \n",
    "    #add results column to the dataframe\n",
    "    project['tr_status'] = y_project\n",
    "    \n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7255c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_date(project):\n",
    "    dates = project['gh_build_started_at'].tolist()\n",
    "    \n",
    "    start_date = dates[0] - datetime.timedelta(days = 1)\n",
    "    end_date = dates[-1] - datetime.timedelta(days = 1)\n",
    "    \n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae6a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_data(p_name, build_ids):\n",
    "    \n",
    "    res_file = '../data/' + p_name + '.csv'\n",
    "    res_project = pd.read_csv(res_file, usecols = ['tr_build_id', 'tr_duration'])\n",
    "    durations = res_project[res_project['tr_build_id'].isin(build_ids)]['tr_duration'].tolist()\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edff3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(p_name, test_builds, test_result, pred_result):\n",
    "    \n",
    "    \n",
    "    durations = get_required_data(p_name, test_builds)\n",
    "    actual_duration = sum(durations)\n",
    "    actual_failures = test_result.count(0)\n",
    "    \n",
    "    total_builds = len(test_builds)\n",
    "    num_of_builds = 0\n",
    "    total_duration = 0\n",
    "    cbf = 0\n",
    "    saved_builds = 0\n",
    "    \n",
    "    batch = []\n",
    "    batch_duration = []\n",
    "    actual_results = []\n",
    "    max_batch_size = 4\n",
    "    \n",
    "    for i in range(len(pred_result)):\n",
    "        if pred_result[i] == 0:\n",
    "            \n",
    "            if test_result[i] == 0:\n",
    "                cbf += 1\n",
    "                \n",
    "            if len(batch) < max_batch_size:\n",
    "                batch.append(pred_result[i])\n",
    "                batch_duration.append(durations[i])\n",
    "                actual_results.append(test_result[i])\n",
    "            \n",
    "            if len(batch) == max_batch_size:\n",
    "                num_of_builds += 1\n",
    "                total_duration += max(batch_duration)\n",
    "                \n",
    "                if 0 in actual_results:\n",
    "                    num_of_builds += 4\n",
    "                    total_duration += sum(batch_duration)\n",
    "        else:\n",
    "            saved_builds += 1\n",
    "            \n",
    "    if len(batch) > 0:\n",
    "        num_of_builds += 1\n",
    "        total_duration += max(batch_duration)\n",
    "        \n",
    "        if 0 in actual_results:\n",
    "            num_of_builds += len(batch)\n",
    "            total_duration += sum(batch_duration)\n",
    "                    \n",
    "    #Delay computation\n",
    "    flag = 0\n",
    "    count = 0\n",
    "    delay = []\n",
    "    for i in range(len(pred_result)):\n",
    "        if flag == 1:\n",
    "            if pred_result[i] == 1:\n",
    "                count += 1\n",
    "            \n",
    "            if pred_result[i] == 0:\n",
    "                delay.append(count)\n",
    "                count = 0\n",
    "                flag = 0\n",
    "                \n",
    "        if test_result[i] != 1:\n",
    "            if pred_result[i] == 1:\n",
    "                flag = 1\n",
    "    delay.append(count)\n",
    "    \n",
    "    print(\"===========================================\")\n",
    "    print('The performance of the model is as follows:')\n",
    "    print('\\t Time saved : {}'.format(total_duration))\n",
    "    print('\\t % Time saved : {}%'.format(100*total_duration/actual_duration))\n",
    "    print('\\t Num. Builds saved : {}%'.format(saved_builds))\n",
    "    print('\\t % Builds saved : {}%'.format(100*saved_builds/total_builds))\n",
    "    print('\\t Num. Builds required : {}'.format(num_of_builds))\n",
    "    print('\\t % Builds required : {}%'.format(100*num_of_builds/total_builds))\n",
    "    print('\\t Num. Failed Builds Identified : {}'.format(cbf))\n",
    "    print('\\t % Failed Builds Identified : {}%'.format(100*cbf/actual_failures))\n",
    "    print('\\t Median Delay Induced : {} builds'.format(median(delay)))\n",
    "    print('\\t Total Delay Induced: {} builds'.format(sum(delay)))\n",
    "    print('\\t Total number of builds: {}'.format(total_builds))\n",
    "    print('\\t Total number of failed builds: {}'.format(actual_failures))\n",
    "    print('\\t Total Duration: {}'.format(actual_duration))\n",
    "    print(\"===========================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5976a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing geoserver\n",
      "0.6 0.84 0.660377358490566 0.7000000000000001 0.6699999999999999\n",
      "0.625 0.6 0.6274509803921569 0.6122448979591836 0.6269230769230769\n",
      "0.6956521739130435 0.5714285714285714 0.6545454545454545 0.6274509803921569 0.656084656084656\n",
      "0.6428571428571429 0.72 0.6666666666666666 0.6792452830188679 0.6676923076923078\n",
      "0.7407407407407407 0.7692307692307693 0.7346938775510204 0.7547169811320754 0.7324414715719063\n",
      "0.75 0.5 0.673469387755102 0.6 0.67\n",
      "0.6111111111111112 0.5238095238095238 0.6222222222222222 0.5641025641025642 0.6160714285714284\n",
      "0.7058823529411765 0.46153846153846156 0.5681818181818182 0.558139534883721 0.5918803418803419\n",
      "0.76 0.7037037037037037 0.7254901960784313 0.7307692307692308 0.7268518518518519\n",
      "0.7692307692307693 0.7407407407407407 0.7547169811320755 0.7547169811320754 0.7549857549857549\n",
      "0.6666666666666666 0.4166666666666667 0.5957446808510638 0.5128205128205129 0.5996376811594203\n",
      "0.7575757575757576 0.8620689655172413 0.7692307692307693 0.8064516129032258 0.7571214392803598\n",
      "0.6774193548387096 0.75 0.6851851851851852 0.7118644067796611 0.6826923076923078\n",
      "0.6363636363636364 0.5833333333333334 0.6170212765957447 0.6086956521739131 0.6177536231884059\n",
      "0.72 0.72 0.7142857142857143 0.72 0.7141666666666666\n",
      "0.8 0.5925925925925926 0.6875 0.6808510638297872 0.701058201058201\n",
      "0.6551724137931034 0.7037037037037037 0.660377358490566 0.6785714285714286 0.6595441595441596\n",
      "0.6842105263157895 0.48148148148148145 0.6226415094339622 0.5652173913043478 0.6253561253561253\n",
      "0.75 0.7741935483870968 0.7272727272727273 0.7619047619047619 0.7204301075268817\n",
      "0.7 0.6176470588235294 0.6206896551724138 0.65625 0.6213235294117647\n",
      "0.8095238095238095 0.53125 0.6545454545454545 0.6415094339622642 0.6786684782608696\n"
     ]
    }
   ],
   "source": [
    "for p_name in project_list:\n",
    "    \n",
    "    print('Processing {}'.format(p_name))\n",
    "    \n",
    "    project = get_complete_data(p_name)\n",
    "    start_date, end_date = get_start_end_date(project)\n",
    "    \n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "    \n",
    "    param_grid = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "    forest = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = forest, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 0)\n",
    "    \n",
    "    phase = 1\n",
    "\n",
    "    while start_date < end_date:\n",
    "        \n",
    "        train_period = 30\n",
    "        test_period = 10\n",
    "        \n",
    "        while True:\n",
    "            train_end = start_date + datetime.timedelta(days = train_period + 1)\n",
    "            test_start = start_date + datetime.timedelta(days = train_period)\n",
    "            test_end = test_start + datetime.timedelta(days = test_period)\n",
    "\n",
    "            #getting data of train & test phase wise\n",
    "            train_data = project[ (project['gh_build_started_at'] > start_date) & (project['gh_build_started_at'] < train_end)]\n",
    "            test_data = project[ (project['gh_build_started_at'] > test_start) & (project['gh_build_started_at'] < test_end)]\n",
    "\n",
    "            #getting 'y' data\n",
    "            train_result = train_data['tr_status'].tolist()\n",
    "            test_result = test_data['tr_status'].tolist()\n",
    "            \n",
    "            if len(train_result) > 100 and len(test_result) > 10 :\n",
    "                break\n",
    "            \n",
    "            if len(train_result) <= 100:\n",
    "                train_period += 20\n",
    "            \n",
    "            if len(test_result) <= 10:\n",
    "                test_period += 20\n",
    "                \n",
    "            \n",
    "        \n",
    "        #dropping build start time column\n",
    "        train_data.drop('gh_build_started_at', inplace=True, axis=1)\n",
    "        test_data.drop('gh_build_started_at', inplace=True, axis=1)\n",
    "        \n",
    "        #add pass_streak to training data:\n",
    "        train_data['num_of_passes'] = get_pass_streak(train_result)\n",
    "        \n",
    "        best_n_estimators = []\n",
    "        best_max_depth = []\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_f1_sample = 0\n",
    "        best_f1_sample_result = 0\n",
    "        best_f1_estimator = 0\n",
    "        best_thresholds = []\n",
    "        \n",
    "        #bootstrap 10 times\n",
    "        for i in range(50):\n",
    "            \n",
    "            file_name = 'rq2_' + p_name + '_' + str(phase) + '_model_' + str(i+1) + '_model.pkl'\n",
    "            sample_train = resample(train_data, replace=True, n_samples=len(train_data))\n",
    "            sample_train_result = sample_train['tr_status']\n",
    "            \n",
    "            build_ids = sample_train['tr_build_id'].tolist()\n",
    "            sample_test = train_data [~train_data['tr_build_id'].isin(build_ids)] \n",
    "            sample_test_result = sample_test['tr_status']\n",
    "            \n",
    "            #dropping result column and build ids column\n",
    "            sample_train.drop('tr_status', inplace=True, axis=1)\n",
    "            sample_train.drop('tr_build_id', inplace=True, axis=1)\n",
    "            sample_test.drop('tr_status', inplace=True, axis=1)\n",
    "            sample_test.drop('tr_build_id', inplace=True, axis=1)\n",
    "            \n",
    "            #training\n",
    "            grid_search.fit(sample_train, sample_train_result)\n",
    "            sample_pred_vals = grid_search.predict_proba(sample_test)\n",
    "            \n",
    "            pred_vals = sample_pred_vals[:, 1]\n",
    "            fpr, tpr, t = roc_curve(sample_test_result, pred_vals)\n",
    "            gmeans = sqrt(tpr * (1-fpr))\n",
    "            ix = argmax(gmeans)\n",
    "            bt = t[ix]\n",
    "            best_thresholds.append(bt)\n",
    "            \n",
    "            final_pred_result = []\n",
    "            #threshold setting\n",
    "            for j in range(len(pred_vals)):\n",
    "                if pred_vals[j] > bt:\n",
    "                    final_pred_result.append(1)\n",
    "                else:\n",
    "                    final_pred_result.append(0)\n",
    "            \n",
    "            accuracy = accuracy_score(sample_test_result, final_pred_result)\n",
    "            precision = precision_score(sample_test_result, final_pred_result)\n",
    "            recall = recall_score(sample_test_result, final_pred_result)\n",
    "            confusion = confusion_matrix(sample_test_result, final_pred_result)\n",
    "            auc_score = roc_auc_score(sample_test_result, final_pred_result)\n",
    "            f1 = f1_score(sample_test_result, final_pred_result)\n",
    "    \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_f1_sample = sample_train\n",
    "                best_f1_sample_result = sample_train_result\n",
    "                best_f1_estimator = grid_search.best_estimator_\n",
    "\n",
    "            print(precision, recall, accuracy, f1, auc_score)\n",
    "            best_n_estimators.append(grid_search.best_params_['n_estimators'])\n",
    "            best_max_depth.append(grid_search.best_params_['max_depth'])\n",
    "        \n",
    "        #completed with bootstrapping \n",
    "        threshold = median(best_thresholds)\n",
    "        n_estimator = median(best_n_estimators)\n",
    "        max_depth = median(best_max_depth)\n",
    "        #retrain on the best \n",
    "        forest = RandomForestClassifier(n_estimators=int(n_estimator), max_depth=int(max_depth))\n",
    "        forest.fit(best_f1_sample, best_f1_sample_result)\n",
    "        \n",
    "        test_builds = test_data['tr_build_id'].tolist()\n",
    "        test_data.drop('tr_build_id', inplace=True, axis=1)\n",
    "        test_data.drop('tr_status', inplace=True, axis=1)\n",
    "        \n",
    "        final_pred_result = []\n",
    "        queue = 0\n",
    "        i = 0\n",
    "        total = len(test_data)\n",
    "        while i < total :\n",
    "            data = test_data.iloc[i]\n",
    "            data['num_of_passes'] = queue\n",
    "            predict = forest.predict_proba([data])\n",
    "            if predict[0][1] > threshold:\n",
    "                final_pred_result.append(1)\n",
    "                queue += 1\n",
    "                i+=1\n",
    "            else:\n",
    "                final_pred_result.append(0)\n",
    "                queue = 0\n",
    "                i += 1\n",
    "                \n",
    "                \n",
    "        print('Individual testing for {}....'.format(p_name))\n",
    "        \n",
    "#         accuracy = accuracy_score(test_result, final_pred_result)\n",
    "#         precision = precision_score(test_result, final_pred_result)\n",
    "#         recall = recall_score(test_result, final_pred_result)\n",
    "#         confusion = confusion_matrix(test_result, final_pred_result)\n",
    "#         auc_score = roc_auc_score(test_result, final_pred_result)\n",
    "#         f1 = f1_score(test_result, final_pred_result)\n",
    "        \n",
    "        print(precision, recall, accuracy, f1, auc_score)\n",
    "        print(confusion)\n",
    "        \n",
    "        compute_performance(p_name, test_builds, test_result, final_pred_result)\n",
    "        \n",
    "#         #testing full set at time\n",
    "#         print('Group testing for {}....'.format(p_name))\n",
    "#         test_data['num_of_passes'] = get_pass_streak(test_result)\n",
    "#         pred_result = forest.predict_proba(test_data)\n",
    "        \n",
    "#         final_pred_result = []\n",
    "#         pred_vals = pred_result[:, 1]\n",
    "#         fpr, tpr, t = roc_curve(test_result, pred_vals)\n",
    "#         gmeans = sqrt(tpr * (1-fpr))\n",
    "#         ix = argmax(gmeans)\n",
    "#         bt = t[ix]\n",
    "#         best_thresholds.append(bt)\n",
    "\n",
    "#         final_pred_result = []\n",
    "#         #threshold setting\n",
    "#         for j in range(len(pred_vals)):\n",
    "#             if pred_vals[j] > bt:\n",
    "#                 final_pred_result.append(1)\n",
    "#             else:\n",
    "#                 final_pred_result.append(0)\n",
    "        \n",
    "# #         accuracy = accuracy_score(test_result, final_pred_result)\n",
    "# #         precision = precision_score(test_result, final_pred_result)\n",
    "# #         recall = recall_score(test_result, final_pred_result)\n",
    "# #         confusion = confusion_matrix(test_result, final_pred_result)\n",
    "# #         auc_score = roc_auc_score(test_result, final_pred_result)\n",
    "# #         f1 = f1_score(test_result, final_pred_result)\n",
    "        \n",
    "#         print(precision, recall, accuracy, f1, auc_score)\n",
    "#         print(confusion)\n",
    "        \n",
    "#         compute_performance(p_name, test_builds, test_result, final_pred_result)\n",
    "        \n",
    "        start_date = test_end\n",
    "        phase += 1\n",
    "        \n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
